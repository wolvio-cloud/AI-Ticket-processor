"""
ai_ticket_processor.py - Optimized parallel ticket processor
Improvements:
- Parallel processing (10 workers) - reduces time from 11s to ~3s per ticket
- Automatic retry logic for API failures
- Daily log files with timestamps
- Proper tag management (preserves Zendesk ML tags)
- PII protection before sending to OpenAI
"""
import requests
import os
import json
import time
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import argparse
from dotenv import load_dotenv
from pii_redactor import PIIRedactor

# Load environment variables
load_dotenv()

# Initialize PII Redactor
redactor = PIIRedactor(preserve_emails=True)

# === CONFIG ===
SUBDOMAIN = os.getenv('ZENDESK_SUBDOMAIN')
EMAIL = os.getenv('ZENDESK_EMAIL')
TOKEN = os.getenv('ZENDESK_API_TOKEN')
OPENAI_KEY = os.getenv('OPENAI_API_KEY')
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)

# Logging with daily rotation
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler(f"{LOG_DIR}/{datetime.now().strftime('%Y%m%d')}.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Retry Session with exponential backoff
def requests_session():
    session = requests.Session()
    retry = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET", "PUT", "DELETE", "OPTIONS", "TRACE", "POST"]
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

session = requests_session()

# === AUTH ===
zendesk_auth = (f"{EMAIL}/token", TOKEN)
openai_headers = {"Authorization": f"Bearer {OPENAI_KEY}", "Content-Type": "application/json"}

# === PRIORITY MAPPING ===
def map_urgency_to_priority(urgency):
    """Map AI urgency levels to Zendesk priority values"""
    mapping = {
        'low': 'low',
        'medium': 'normal',  # Zendesk uses 'normal' not 'medium'
        'high': 'high'
    }
    return mapping.get(urgency, 'normal')

# === AI PROMPT ===
PROMPT = """
You are a senior support analyst. Analyze this ticket and return ONLY valid JSON:

Ticket: {description}

{{
  "summary": "1-sentence summary",
  "root_cause": "bug|refund|feature|other",
  "urgency": "low|medium|high",
  "sentiment": "positive|neutral|negative"
}}
"""

# === OPENAI ANALYSIS ===
def analyze_with_openai(description):
    """Analyze ticket with OpenAI (with retry logic + PII protection)"""
    start = time.time()
    
    # STEP 1: Redact PII before sending to OpenAI
    redaction_result = redactor.redact(description)
    clean_description = redaction_result['redacted_text']
    
    # Log if PII was found
    if redaction_result['has_pii']:
        logger.warning(f"[PII] Detected and redacted: {redaction_result['redactions']}")
    
    # STEP 2: Send clean text to OpenAI
    payload = {
        "model": "gpt-4o-mini",
        "messages": [{"role": "user", "content": PROMPT.format(description=clean_description)}],
        "response_format": {"type": "json_object"}
    }
    try:
        resp = session.post(
            "https://api.openai.com/v1/chat/completions",
            json=payload,
            headers=openai_headers,
            timeout=30
        )
        resp.raise_for_status()
        result = resp.json()
        analysis = json.loads(result['choices'][0]['message']['content'])
        return {
            "success": True,
            "analysis": analysis,
            "processing_time": round(time.time() - start, 2),
            "pii_protected": redaction_result['has_pii'],
            "redactions": redaction_result['redactions']
        }
    except Exception as e:
        logger.error(f"OpenAI failed: {e}")
        return {"success": False, "error": str(e), "processing_time": round(time.time() - start, 2)}


# === ZENDESK UPDATE (FIXED TAG HANDLING) ===
def update_ticket(ticket_id, analysis):
    """Update Zendesk ticket with AI analysis (preserves existing tags)"""
    start = time.time()
    url = f"https://{SUBDOMAIN}.zendesk.com/api/v2/tickets/{ticket_id}.json"
    
    try:
        # Step 1: Fetch existing tags
        resp_get = session.get(url, auth=zendesk_auth, timeout=10)
        resp_get.raise_for_status()
        current_ticket = resp_get.json()['ticket']
        existing_tags = current_ticket.get('tags', [])
        
        # Step 2: Create our AI tags
        ai_tags = [
            "ai_processed",
            f"ai_{analysis['root_cause']}",
            f"ai_{analysis['urgency']}",
            f"ai_{analysis['sentiment']}"
        ]
        
        # Step 3: Combine tags (avoid duplicates)
        all_tags = list(set(existing_tags + ai_tags))
        
        # Step 4: Create internal comment
        comment_body = f"""AI Analysis:

Summary: {analysis['summary']}
Root Cause: {analysis['root_cause']}
Urgency: {analysis['urgency']}
Sentiment: {analysis['sentiment']}
"""
        
        # Step 5: Update ticket
        payload = {
            "ticket": {
                "comment": {"body": comment_body, "public": False},
                "tags": all_tags,
                "priority": map_urgency_to_priority(analysis['urgency'])
            }
        }
        
        resp_put = session.put(url, json=payload, auth=zendesk_auth, timeout=10)
        resp_put.raise_for_status()
        
        logger.info(f"Ticket {ticket_id} updated with tags: {ai_tags}")
        return {"updated": True, "time": round(time.time() - start, 2)}
        
    except requests.exceptions.HTTPError as e:
        # Log the response content for debugging
        error_detail = e.response.text if hasattr(e, 'response') else str(e)
        logger.error(f"Zendesk update failed (ID {ticket_id}): {e} - {error_detail}")
        return {"updated": False, "error": str(e), "time": round(time.time() - start, 2)}
    except Exception as e:
        logger.error(f"Zendesk update failed (ID {ticket_id}): {e}")
        return {"updated": False, "error": str(e), "time": round(time.time() - start, 2)}


# === PROCESS SINGLE TICKET ===
def process_ticket(ticket):
    """Process one ticket through the entire pipeline"""
    ticket_id = ticket['id']
    description = ticket.get('description', '') or ticket.get('subject', '')
    
    if not description.strip():
        return {"ticket_id": ticket_id, "success": False, "error": "No description"}
    
    logger.info(f"Processing ticket {ticket_id}")
    
    # Step 1: Analyze with AI
    ai_result = analyze_with_openai(description)
    if not ai_result["success"]:
        return {**ai_result, "ticket_id": ticket_id, "updated": False}
    
    # Step 2: Update Zendesk
    update_result = update_ticket(ticket_id, ai_result["analysis"])
    
    return {
        "ticket_id": ticket_id,
        "success": True,
        "analysis": ai_result["analysis"],
        "processing_time": ai_result["processing_time"],
        "updated": update_result["updated"],
        "pii_protected": ai_result.get("pii_protected", False),
        "redactions": ai_result.get("redactions", {})
    }


# === MAIN ===
def main(limit=50):
    """Main processing function with parallel execution"""
    start_total = time.time()
    
    logger.info(f"Starting batch processing (limit: {limit})")
    print(f"AI TICKET PROCESSOR - Started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    # Fetch tickets using search API (more reliable)
    url = f"https://{SUBDOMAIN}.zendesk.com/api/v2/search.json"
    params = {
        'query': 'type:ticket',
        'sort_by': 'updated_at',
        'sort_order': 'desc'
    }
    
    try:
        resp = session.get(url, params=params, auth=zendesk_auth, timeout=10)
        resp.raise_for_status()
        tickets = resp.json()['results'][:limit]
        print(f"Successfully fetched {len(tickets)} tickets\n")
    except Exception as e:
        logger.critical(f"Failed to fetch tickets: {e}")
        print(f"ERROR: Failed to fetch tickets - {e}")
        return
    
    # Process tickets in parallel
    results = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(process_ticket, ticket): ticket for ticket in tickets}
        
        for i, future in enumerate(as_completed(futures), 1):
            result = future.result()
            results.append(result)
            
            # Print progress
            status = "SUCCESS" if result.get("success") else "FAILED"
            print(f"[{i}/{len(tickets)}] Ticket #{result['ticket_id']}: {status}")
    
    # Calculate statistics
    total_time = round(time.time() - start_total, 2)
    success = sum(1 for r in results if r.get("success"))
    failed = len(tickets) - success
    avg_time = round(sum(r.get("processing_time", 0) for r in results) / len(results), 2) if results else 0
    
    # PII Protection Stats
    tickets_with_pii = sum(1 for r in results if r.get("pii_protected", False))
    total_redactions = {}
    for r in results:
        for pii_type, count in r.get("redactions", {}).items():
            total_redactions[pii_type] = total_redactions.get(pii_type, 0) + count
    
    # Summary
    summary = {
        "timestamp": datetime.now().isoformat(),
        "total": len(tickets),
        "processed": success,
        "failed": failed,
        "avg_time_per_ticket": avg_time,
        "total_time": total_time,
        "cost_estimate": round(len(tickets) * 0.001, 3),
        "pii_protection": {
            "tickets_with_pii": tickets_with_pii,
            "total_redactions": sum(total_redactions.values()),
            "by_type": total_redactions
        },
        "results": results
    }
    
    # Print summary
    print("\n" + "="*60)
    print("BATCH PROCESSING COMPLETE")
    print("="*60)
    print(f"Total Tickets:    {len(tickets)}")
    print(f"Processed:        {success}")
    print(f"Failed:           {failed}")
    print(f"Avg Time:         {avg_time}s per ticket")
    print(f"Total Time:       {total_time}s ({total_time/60:.1f} minutes)")
    print(f"Cost Estimate:    ${summary['cost_estimate']}")
    print("="*60)
    print("PII PROTECTION SUMMARY")
    print("="*60)
    print(f"Tickets with PII: {tickets_with_pii}")
    print(f"Total Redactions: {sum(total_redactions.values())}")
    if total_redactions:
        for pii_type, count in total_redactions.items():
            print(f"  - {pii_type}: {count}")
    else:
        print("  No PII detected")
    print("="*60)
    
    # Save results to JSON
    json_file = f"{LOG_DIR}/results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(json_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nResults saved to: {json_file}")
    logger.info(f"Batch complete: {success}/{len(tickets)} | Avg: {avg_time}s | Total: {total_time}s")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='AI Ticket Processor - Optimized')
    parser.add_argument("--limit", type=int, default=50, help="Number of tickets to process")
    args = parser.parse_args()
    
    main(args.limit)